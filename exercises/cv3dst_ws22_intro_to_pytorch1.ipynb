{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7x57SEGcohj"
   },
   "source": [
    "# PyTorch Introduction\n",
    "\n",
    "Welcome to the introduction of PyTorch. PyTorch is a scientific computing package targeted for two main purposes: \n",
    "\n",
    "1. A replacement for NumPy with the ability to use the power of GPUs.\n",
    "\n",
    "2. A deep learning framework that enables the flexible and swift building of neural network models.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n",
    "### Goals of this tutorial\n",
    "\n",
    "- Getting to know PyTorch and understanding how it is different from numpy\n",
    "\n",
    "- Understanding PyTorch's Tensor and Pytorch's Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ0lMLiIcohm"
   },
   "source": [
    "## Enable GPUs on Colab\n",
    "\n",
    "Having a library that has GPU support is one thing, the other is actually owning the hardware. Alternatively, you can use google colab though we have to manually enable it.\n",
    "\n",
    "To enable GPU support in Google Colab go to `Menu > Runtime > Change runtime type` and enable the GPU hardware accelerator to speed up your trainings considerably. However, this functionality might not be available at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTGDmgxfcohn"
   },
   "source": [
    "\n",
    "# Installing PyTorch\n",
    "\n",
    "Pytorch provides support for accelerating computation using CUDA enabled GPU's. If your workstation has an NVIDIA GPU, install PyTorch along with the CUDA component.\n",
    "\n",
    "#### Install [PyTorch](https://pytorch.org/) and [torchvision](https://github.com/pytorch/vision)\n",
    "\n",
    "For this class we will use the current Pytorch version 1.11. To install, please uncomment and run the proper line in the upcoming cell depending on your operating system (and CUDA setup). We won't go into details of installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wFLMsKurcohn"
   },
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "# For google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# For Linux and probably Windows (CPU)\n",
    "# !{sys.executable} -m pip install torch==1.11.0+cpu torchvision==0.12.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# For Linux and probably Windows (Prerequisites: Nvidia GPU + CUDA toolkit 11.3)\n",
    "# !{sys.executable} -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "\n",
    "# For OS X/Mac\n",
    "# !{sys.executable} -m pip install torch==1.11.0 torchvision==0.12.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LETrKHnbcoho"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Nvidia GPU</b>\n",
    "    <p>If you have a rather recent Nvidia GPU, you can go ahead and install the CUDA toolkit together with a current version of cudnn (though it is possible to use other versions as long as you build it yourself). Afterwards, you can run the respective line in the cell above.</p>\n",
    "    <p>There are multiple setups on how to install those on both Linux and Windows, but it depends on your setup. If you want to utilize your GPU you have to go through those steps. Use the forum for help if you get stuck.</p>\n",
    "    <br>\n",
    "    <b>Google Colab Pytorch Installation Time</b>\n",
    "    <p>Google colab might use an older/newer version of pytorch. Since we are mostly using defualt functionality, you should be fine by using the default colab version to avoid the long installation time at your own risk.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STeqjUu9coho"
   },
   "source": [
    "#### Checking PyTorch Installation and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Scn7PTB-cohp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4bd71d56-e3ca-40a2-d843-962ddb74c6ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version Installed: 1.10.2+cu113\n",
      "Torchvision version Installed: 0.11.3+cu113\n",
      "\n",
      "you are using an another version of PyTorch. We expect PyTorch 1.11.0. You may continue using your version but it might cause dependency and compatibility issues.\n",
      "you are using an another version of torchvision. We expect torchvision 0.12. You can continue with your version but it might cause dependency and compatibility issues.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(f\"PyTorch version Installed: {torch.__version__}\\nTorchvision version Installed: {torchvision.__version__}\\n\")\n",
    "if not torch.__version__.startswith(\"1.11\"):\n",
    "    print(f\"you are using an another version of PyTorch. We expect PyTorch 1.11.0. You may continue using your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "if not torchvision.__version__.startswith(\"0.12\"):\n",
    "    print(f\"you are using an another version of torchvision. We expect torchvision 0.12. You can continue with your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "# set printing options for nice output in this notebook\n",
    "torch.set_printoptions(profile=\"short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuuE8p-wcohq"
   },
   "source": [
    "That's the end of installation. Let's dive right into PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAh4wPMGcohq"
   },
   "source": [
    "\n",
    "The following block imports the required packages for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QnzcABRjcohq"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33hiyTpocohr"
   },
   "source": [
    "## 1. Tensors\n",
    "\n",
    "[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) is the central class of PyTorch.\n",
    "Tensors are similar to NumPyâ€™s ndarrays. The advantage of using Tensors is that \n",
    "\n",
    "* one can easily transfer them from CPU to GPU and therefore computations on tensors can be accelerated with a GPU.\n",
    "* they store additionally the gradients, if requires_grad=True is set, which is needed for efficient backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOCRPTzTcohs"
   },
   "source": [
    "## 1.1 Initializing Tensor\n",
    "Let us construct a NumPy array and a tensor of shape (2,3) directly from data values.\n",
    "\n",
    "The interfaces are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "t8q4pSGHcohs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "801613d9-5e69-43db-9cc6-d01d1b07e79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable array_np:\n",
      "Datatype: <class 'numpy.ndarray'>\n",
      "Shape: (2, 3)\n",
      "Values:\n",
      "[[1 2 3]\n",
      " [5 6 7]]\n",
      "\n",
      "\n",
      "Variable array_ts:\n",
      "Datatype <class 'torch.Tensor'>\n",
      "Shape: torch.Size([2, 3])\n",
      "Values:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Numpy Array\n",
    "array_np = np.array([[1,2,3],[5,6,7]]) #NumPy array\n",
    "# Initializing the Tensor\n",
    "array_ts = torch.tensor([[1,2,3],[4,5,6]]) # Tensor\n",
    "\n",
    "print(f\"Variable array_np:\\nDatatype: {type(array_np)}\\nShape: {array_np.shape}\")\n",
    "print(f\"Values:\\n{array_np}\")\n",
    "print(f\"\\n\\nVariable array_ts:\\nDatatype {type(array_ts)}\\nShape: {array_ts.shape}\")\n",
    "print(f\"Values:\\n{array_ts.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBMMSrqicohs"
   },
   "source": [
    "## 1.2 Conversion between NumPy array and Tensor\n",
    "\n",
    "The conversion between NumPy ndarray and PyTorch tensor is quite easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "33JMr6U4cohs"
   },
   "outputs": [],
   "source": [
    "# Conversion\n",
    "array_np = np.array([1, 2, 3])\n",
    "# Conversion from  a numpy array to a Tensor\n",
    "array_ts_2 = torch.from_numpy(array_np)\n",
    "\n",
    "# Conversion from  Tensor to numpy array\n",
    "array_np_2 = array_ts_2.numpy() \n",
    "\n",
    "# Change a value of the np_array\n",
    "array_np_2[1] = -1 \n",
    "\n",
    "# Changes in the numpy array will also change the values in the tensor\n",
    "assert(array_np[1] == array_np_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gukIaJnGcoht"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b></b> During the conversion, both ndarrays and the Tensor share the same memory address. Changes in value of one will\n",
    "affect the other.</div>\n",
    "\n",
    "## 1.3 Operations on Tensor\n",
    "\n",
    "### 1.3.1 Indexing\n",
    "\n",
    "We can use the NumPy array-like indexing for Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_ts[:2,:2][:, 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jhHWUo6kcoht",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "22dd9b03-8d28-47b9-da95-727f046c924f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [0, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Let us take the first two columns from the original tensor array and save it in a new one\n",
    "b = array_ts[:2, :2] \n",
    "\n",
    "# Let's assign the value of first column of the new variable to be zero \n",
    "b[:, 0] = 0 \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgVvfUpOcoht"
   },
   "source": [
    "We will now select elements which satisfy a particular condition. In this example, let's find those elements of tensor which are array greater than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Joe5FuErcoht",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7a3b2366-962e-4b99-cc3e-749d8ab82455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Index of the elements with value greater than one\n",
    "mask = array_ts > 1 \n",
    "new_array = array_ts[mask]\n",
    "print(new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vK3ipAyjcoht"
   },
   "source": [
    "Let's try performing the same operation in a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QYMFNRdccohu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e86e02c0-dbe1-442c-d9c7-df1748d5c379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "c = array_ts[array_ts>1]\n",
    "\n",
    "# Is the result same as the array from the previous cell?\n",
    "print(c == new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c_gzrRacohu"
   },
   "source": [
    "### 1.3.2 Mathematical operations on Tensor\n",
    "\n",
    "\n",
    "#### Element-wise operations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Nhvy0099cohu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dd1101be-4807-4191-95f4-4c2835b1001a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y: \n",
      "[[ 6  8]\n",
      " [10 12]]\n",
      "x + y: \n",
      "[[ 6  8]\n",
      " [10 12]]\n",
      "x + y: \n",
      "[[ 6.  8.]\n",
      " [10. 12.]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]])\n",
    "y = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "# Addition - Syntax 1\n",
    "print(f\"x + y: \\n{(x + y).cpu().numpy()}\")\n",
    "\n",
    "# Addition - Syntax 2\n",
    "print(f\"x + y: \\n{torch.add(x, y).cpu().numpy()}\")\n",
    "\n",
    "# Addition - Syntax 3\n",
    "result_add = torch.empty(2, 2)\n",
    "torch.add(x, y, out=result_add)\n",
    "print(f\"x + y: \\n{result_add.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saasrrq8cohu"
   },
   "source": [
    "Note: We only added `.cpu().numpy()` to receive a better formatted print statement.\n",
    "\n",
    "Similar syntax holds for other element-wise operations such as subtraction and multiplication.\n",
    "\n",
    "When dividing two integers in NumPy as well PyTorch, the result is always a **float**.   \n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1GncNvh0cohv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bed28fcb-2026-4aeb-d7df-91335919860f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x / y: \n",
      "[[0.2        0.33333333]\n",
      " [0.42857143 0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "x_np = np.array([[1,2],[3,4]])\n",
    "y_np = np.array([[5,6],[7,8]])\n",
    "print(f\"x / y: \\n{x_np / y_np}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Matrix Multiplication\n",
    "\n",
    "PyTorch offers different options for doing matrix matrix multiplication.\n",
    "\n",
    "If you want to do matrix mupliplication with more then two tensors you can use [torch.einsum()](https://pytorch.org/docs/stable/generated/torch.einsum.html). Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention."
   ],
   "metadata": {
    "id": "kRR1jQXFmKIr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tensor1 = torch.randn(3, 3)\n",
    "tensor2 = torch.randn(3)\n",
    "\n",
    "# Matrix Multiplication - Syntax 1\n",
    "output1 = tensor1 @ tensor2\n",
    "# Matrix Multiplication - Syntax 2\n",
    "output2 = torch.matmul(tensor1, tensor2)\n",
    "# Matrix Multiplication - Syntax 3\n",
    "output3 = torch.einsum(\"ij,j->i\", tensor1, tensor2)\n",
    "\n",
    "print(f\"Matrix mutlplication\\nInputs:\\n{tensor1.cpu().numpy()}\\nand\\n{tensor2.cpu().numpy()} \\n\\n\",\n",
    "      f\"Output1: \\n{output1.cpu().numpy()}\\n\",\n",
    "      f\"Output2: \\n{output2.cpu().numpy()}\\n\",\n",
    "      f\"Output3: \\n{output3.cpu().numpy()}\")\n",
    "\n",
    "assert output1.equal(output2)\n",
    "assert output2.equal(output3)"
   ],
   "metadata": {
    "id": "ig8oaYH5xkzn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e495e912-611e-452e-e240-9dd021eec870"
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix mutlplication\n",
      "Inputs:\n",
      "[[ 0.48588315 -0.8225588   0.14899746]\n",
      " [ 0.69823587 -0.8034909   0.16900674]\n",
      " [ 0.8192266  -0.43813497  0.12787765]]\n",
      "and\n",
      "[ 0.57209957 -0.61691636  0.88316196] \n",
      "\n",
      " Output1: \n",
      "[0.9170124 1.0444074 0.8519085]\n",
      " Output2: \n",
      "[0.9170124 1.0444074 0.8519085]\n",
      " Output3: \n",
      "[0.9170124 1.0444074 0.8519085]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Doing matrix multiplication with more than two tensors."
   ],
   "metadata": {
    "id": "RlYwd7_ZEePs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3, 3)\n",
    "tensor3 = torch.randn(3)\n",
    "# Matrix Multiplication - Syntax 1\n",
    "output1 = tensor1 @ tensor2 @ tensor3\n",
    "# Matrix Multiplication - Syntax 2\n",
    "output2 = torch.einsum(\"i,ij,j\", tensor1, tensor2, tensor3)\n",
    "\n",
    "print(f\"Chain multiplication:\\n{output1}\\n{output2}\")"
   ],
   "metadata": {
    "id": "Z-iOgkZTEeph",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "39b1eaae-e66a-412c-98af-ce09fa88d026"
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain multiplication:\n",
      "-1.0988529920578003\n",
      "-1.0988529920578003\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When working with PyTorch operations are often done over a batch. PyTorch offers batching and broadcasting for matrix multiplication."
   ],
   "metadata": {
    "id": "GOQSuNIvALgT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix x vector multiplication:\n",
      " Input shapes:\n",
      " [3, 4] and [4]\n",
      " Output shape:\n",
      " [3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(\n",
    "    f\"matrix x vector multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# vector x vector\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "print(\n",
    "    f\"vector x vector multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\", \n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")\n",
    "\n",
    "# matrix x vector\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(\n",
    "    f\"matrix x vector multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")\n",
    "\n",
    "# batched matrix x broadcasted vector\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(\n",
    "    f\"batched matrix x broadcasted vector multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")\n",
    "\n",
    "# batched matrix x batched matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(10, 4, 5)\n",
    "print(\n",
    "    f\"batched matrix x batched matrix multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")\n",
    "\n",
    "# batched matrix x broadcasted matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4, 5)\n",
    "print(\n",
    "    f\"batched matrix x broadcasted matrix multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")"
   ],
   "metadata": {
    "id": "QntOHF3vmJS5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cc625008-1d09-4924-cd2f-cb3d73738038"
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector x vector multiplication:\n",
      " Input shapes:\n",
      " [3] and [3]\n",
      " Output shape:\n",
      " []\n",
      "\n",
      "matrix x vector multiplication:\n",
      " Input shapes:\n",
      " [3, 4] and [4]\n",
      " Output shape:\n",
      " [3]\n",
      "\n",
      "batched matrix x broadcasted vector multiplication:\n",
      " Input shapes:\n",
      " [10, 3, 4] and [4]\n",
      " Output shape:\n",
      " [10, 3]\n",
      "\n",
      "batched matrix x batched matrix multiplication:\n",
      " Input shapes:\n",
      " [10, 3, 4] and [10, 4, 5]\n",
      " Output shape:\n",
      " [10, 3, 5]\n",
      "\n",
      "batched matrix x broadcasted matrix multiplication:\n",
      " Input shapes:\n",
      " [10, 3, 4] and [4, 5]\n",
      " Output shape:\n",
      " [10, 3, 5]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For addtional mathematical operations check out the [PyTorch](https://pytorch.org/docs/stable/index.html) documentation"
   ],
   "metadata": {
    "id": "D-zBMdtDme2y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Gradients\n",
    "\n",
    "We create two tensors a and b with requires_grad=True. This signals to `autograd` that every operation on them should be tracked. We create another tensor ``Q`` from ``a`` and ``b``. \n",
    "\n",
    "$Q = 3a^3 - b^2$\n",
    "\n",
    "`autograd` then let us compute the gradient of ``Q`` with respect to ``a`` and ``b``. In this case\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial a} = 9a^2$\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial b} = -2b$"
   ],
   "metadata": {
    "id": "INW-ySMNtHj_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "q  = 3*a**3 - b**2\n",
    "\n",
    "q.sum().backward()\n",
    "print(a.grad.cpu())\n",
    "print(b.grad.cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "# compute the a function with the pytorch tensors\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "# call backward on a function to compute the gradient\n",
    "Q.sum().backward()\n",
    "print(f\"Gradients:\\na:\\n{a.grad.cpu().numpy()}\\nb:\\n{b.grad.cpu().numpy()}\")"
   ],
   "metadata": {
    "id": "IStjDtelAzrz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ec4a724b-63e6-4a34-d372-aa758767ad68"
   },
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:\n",
      "a:\n",
      "[36. 81.]\n",
      "b:\n",
      "[-12.  -8.]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Disable the gradient computation for single tensors by setting `requires_grad=False`."
   ],
   "metadata": {
    "id": "j1XnWDs4Cu21"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=False)\n",
    "\n",
    "# compute the a function with the pytorch tensors\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "# call backward on a function to compute the gradient\n",
    "Q.sum().backward()\n",
    "print(f\"Gradients:\\na:\\n{a.grad.cpu().numpy()}\\nb:\\n{b.grad}\")"
   ],
   "metadata": {
    "id": "9pRBBSHmCO-J",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "89ac0157-5112-47d8-e2d7-2316c6029798"
   },
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:\n",
      "a:\n",
      "[36. 81.]\n",
      "b:\n",
      "None\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When doing evaluations you can wrap a code block in \n",
    "`with torch.no_grad()`\n",
    "to prevent gradient computation."
   ],
   "metadata": {
    "id": "Jz0vopIUCQ2S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "  a = torch.tensor([2., 3.], requires_grad=True)\n",
    "  b = torch.tensor([6., 4.], requires_grad=False)\n",
    "\n",
    "  # compute the a function with the pytorch tensors\n",
    "  Q = 3*a**3 - b**2\n",
    "\n",
    "  # call backward with torch.no_grad() enabled results in a runtime error\n",
    "  try:\n",
    "    Q.sum().backward()\n",
    "  except RuntimeError as e:\n",
    "    print(f\"RuntimeError: {e}\")\n",
    "  print(f\"Gradients:\\na:\\n{a.grad}\\nb:\\n{b.grad}\")"
   ],
   "metadata": {
    "id": "HS0MevWkCRwR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "80db4f58-c5d5-4ee7-d54f-bdc192d70007"
   },
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "Gradients:\n",
      "a:\n",
      "None\n",
      "b:\n",
      "None\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbjR6ymlcohv"
   },
   "source": [
    "## 1.5 Devices\n",
    "\n",
    "When training a neural network, it is important to make sure that all the required tensors as well as the model are on the same device. Tensors can be moved between the CPU and GPU using `.to` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPGi018ccohv"
   },
   "source": [
    "Let us check if a GPU is available. If it is available, we will assign it to `device` and move the tensor `x` to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_VrcsaNncohv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7c98fe17-85c9-4bef-fe6f-03b69a0a6f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Original device: cpu\n",
      "Current device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(f\"Original device: {x.device}\") # \"cpu\"\n",
    "\n",
    "tensor = x.to(device)\n",
    "print(f\"Current device: {tensor.device}\") #\"cpu\" or \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io5sMoGecohv"
   },
   "source": [
    "So `x` has been moved on to a CUDA device for those who have a GPU; otherwise it's still on the CPU.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Try including the <b>.to(device)</b> calls in your codes. It is then easier to port the code to run on a GPU.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.6 Timing with PyTorch\n",
    "\n",
    "Timing CPU-only operations can be done with standard python timing operations, e.g. timeit.\n",
    "\n",
    "Since CUDA is asynchronous, timing GPU operations needs some additional tools. One option uses CUDA events. Timing the matrix multiplication is done by sandwiching the call between CUDA events.\n",
    "\n",
    "Other timing options that use the PyTorch [autograd profiler](https://pytorch.org/docs/stable/autograd.html?highlight=autograd%20profiler#torch.autograd.profiler.profile) are also possible."
   ],
   "metadata": {
    "id": "yv0_lLRjm4Gl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# create random variables to do matrix multiplication with\n",
    "A = torch.randn((10, 10000, 10000), device=\"cpu\")\n",
    "b = torch.randn((10000, 1), device=\"cpu\")\n",
    "\n",
    "start_cpu = time.perf_counter()\n",
    "results_cpu = A @ b\n",
    "end_cpu = time.perf_counter()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "print(f\"Time with cpu in sec: \\n{end_cpu - start_cpu}\")\n",
    "\n",
    "A.to(device)\n",
    "b.to(device)\n",
    "\n",
    "# create a start and end cuda event used for timing\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "results_gpu = A @ b\n",
    "end.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Timing with {device} in sec: \\n{start.elapsed_time(end) / 1000}\")"
   ],
   "metadata": {
    "id": "4HekdEoCqoCa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "be08f5dc-ba27-4399-a662-1fbd28397671"
   },
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with cpu in sec: \n",
      "0.1541735000000699\n",
      "Timing with cuda in sec: \n",
      "0.12709046173095703\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbf148f741bdcfda68412a690aa89cb477d7b565ce205d715d0eac6f68c7216c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "VQ0lMLiIcohm",
    "33hiyTpocohr"
   ]
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
